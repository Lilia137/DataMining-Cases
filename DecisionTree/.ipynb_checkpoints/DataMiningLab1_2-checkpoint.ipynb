{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455a8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from math import log\n",
    "import operator\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287e2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEntropy(dataSet):\n",
    "    total = dataSet.shape[0]\n",
    "    lableCount = dataSet.groupby([\"is_attributed\"]).size()\n",
    "    Entropy = 0\n",
    "    for key in lableCount.index:\n",
    "        P = float(lableCount[key]/total)\n",
    "        Entropy -= P*log(P,2)\n",
    "    return Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f6d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByFeature(dataSet,feature,value):\n",
    "    left = dataSet[(dataSet[feature] <= value)]\n",
    "    right = dataSet[(dataSet[feature] >= value)]\n",
    "    return left,right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d111df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到数据集中出现次数最多的类型\n",
    "def findMajorClass(dataSet):\n",
    "    lableCount = dataSet.groupby([\"is_attributed\"]).size()\n",
    "    maxClass = lableCount.max()\n",
    "    for subClass in lableCount.index:\n",
    "        if lableCount[subClass] == maxClass:\n",
    "            return subClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a271899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet为分割后的dataSet,求出dataSet的长度和oldEntropy,计算feature中的所有feature中，信息熵最小的那个，即令maxGain最大的\n",
    "# 怎么求feature的信息熵呢，连续型进行二分:for循环遍历feature（feature），对某个feature1，先对他的值进行排序，然后每两个点之间取中间值，for\n",
    "# 循环遍历这一组中间值（value），利用splitByFeature函数，将它分成两组left,right之后，分别求这两组的熵，分别乘所占比例后相加，加入到FeatureEntropy。\n",
    "# 循环完成后找最小FeatureEntropy，加入到minFeatureEntropy数组中,该中间值加入minEntropySplit中,最终feature遍历完成后，\n",
    "# 找到minFeatureEntropy中最小的一个，并记录下索引，返回该Feature，split的值，大dataSet的Entropy，sample数，value[0,1],class\n",
    "\n",
    "def findBestFeature(dataSet, features, depth, minSamples):\n",
    "    minFeatureEntropy = np.array([])    # 用于存储每一个feature的最小熵值 （按顺序包含所有feature的）\n",
    "    minSplitValue = np.array([])    # 用于存储使得每一个feature熵值最小的分隔值（按顺序包含所有feature的）\n",
    "    # 该节点的熵值\n",
    "    oldEntropy = computeEntropy(dataSet)\n",
    "    # 该节点dataSet中的sample数\n",
    "    sample_num = len(dataSet)\n",
    "    # 该节点中两种类别的sample数组\n",
    "    lableCount = dataSet.groupby([\"is_attributed\"]).size()\n",
    "    # 该节点的类别\n",
    "    node_class = findMajorClass(dataSet)\n",
    "\n",
    "    # 如果数据集是纯的,停止分支\n",
    "    if len(dataSet.groupby([\"is_attributed\"]).size()) == 1:\n",
    "        if dataSet.groupby([\"is_attributed\"]).size().index[0] == 0:\n",
    "            zero_num = lableCount[0]\n",
    "            one_num = 0\n",
    "        elif dataSet.groupby([\"is_attributed\"]).size().index[0] == 1:\n",
    "            zero_num = 0\n",
    "            one_num = lableCount[1]\n",
    "        return -1,-1,oldEntropy, sample_num, zero_num, one_num, node_class\n",
    "\n",
    "    # 如果节点的记录数小于minSamples时，则停止分支\n",
    "    elif sample_num <= minSamples:\n",
    "        if dataSet.groupby([\"is_attributed\"]).size().index[0] == 0:\n",
    "            zero_num = lableCount[0]\n",
    "            one_num = 0\n",
    "        elif dataSet.groupby([\"is_attributed\"]).size().index[0] == 1:\n",
    "            zero_num = 0\n",
    "            one_num = lableCount[1]\n",
    "        return -1,-1,oldEntropy, sample_num, zero_num, one_num, node_class\n",
    "\n",
    "    # 如果到了要求的最大深度\n",
    "    elif depth == 0:\n",
    "        zero_num = lableCount[0]\n",
    "        one_num = lableCount[1]\n",
    "        return -1, -1, oldEntropy, sample_num, zero_num, one_num, node_class\n",
    "\n",
    "    # 如果既没有实现数据集是纯的，记录数大于minSamples，也没到最大深度，则继续进行分割\n",
    "    else:\n",
    "        for feature in features:\n",
    "            middle = np.array([])  # 中间值\n",
    "            featureEntropy = np.array([])   # 某一个feature，在不同分割情况下的熵值\n",
    "\n",
    "            featureValue = dataSet[feature]\n",
    "            featureValue = sorted(pd.unique(featureValue))\n",
    "            if len(featureValue)==1:\n",
    "                middle = np.append(middle, featureValue[0])\n",
    "            else:\n",
    "                for i in range(0,len(featureValue)-1):\n",
    "                    middle = np.append(middle,(featureValue[i]+featureValue[i+1])/2)\n",
    "            for middleValue in middle:\n",
    "                left,right = splitByFeature(dataSet,feature,middleValue)\n",
    "                leftEntropy = computeEntropy(left)\n",
    "                rightEntropy = computeEntropy(right)\n",
    "                left_P = len(left)/len(dataSet)\n",
    "                right_P = len(right)/len(dataSet)\n",
    "                newEntropy = left_P * leftEntropy + right_P * rightEntropy\n",
    "                featureEntropy = np.append(featureEntropy,newEntropy)\n",
    "            # sub_min_index为某一个feature最小熵值在数组中的索引，sub_min_Entropy为该feature的最小熵值，featureEntropy,middle是对应的\n",
    "            sub_min_index, sub_min_Entropy = min(enumerate(featureEntropy), key=operator.itemgetter(1))\n",
    "            minSplitValue = np.append(minSplitValue,middle[sub_min_index])\n",
    "            minFeatureEntropy = np.append(minFeatureEntropy, sub_min_Entropy)\n",
    "        # minFeatureEntropy，minSplitValue是对应的\n",
    "        min_index, min_Entropy = min(enumerate(minFeatureEntropy),key=operator.itemgetter(1))\n",
    "        # 该节点中两种类别的sample数\n",
    "        zero_num = lableCount[0]\n",
    "        one_num = lableCount[1]\n",
    "        # 最佳分割特征\n",
    "        bestFeature = features[min_index]\n",
    "        # 最佳分隔值\n",
    "        bestFeatureSplit = minSplitValue[min_index]\n",
    "        return bestFeature, bestFeatureSplit, oldEntropy, sample_num, zero_num, one_num, node_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef1461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以可视化的树结构，字典\n",
    "import math\n",
    "def creatTree_plot(dataSet,features,depth,minSamples):\n",
    "\n",
    "    bestFeature, split, oldEntropy, sample_num, zero_num, one_num, node_class = findBestFeature(dataSet,features,depth,minSamples)\n",
    "    \n",
    "    if node_class==0:\n",
    "        the_class='\\'-\\''\n",
    "    elif node_class==1:\n",
    "        the_class='\\'+\\''\n",
    "    \n",
    "    # 如果到了叶子节点\n",
    "    if bestFeature == -1:\n",
    "        content = 'entropy:'+str(round(oldEntropy,3))+'\\n'+'sample:'+str(sample_num)+'\\n'+'values:['+str(zero_num)+','+str(one_num)+']'+'\\n'+'class:'+str(the_class)\n",
    "        mytree = {content:{}}\n",
    "        return mytree\n",
    "    \n",
    "    content = str(bestFeature)+'<='+str(split)+'?\\n'+'entropy:'+str(round(oldEntropy,3))+'\\n'+'sample:'+str(sample_num)+'\\n'+'values:['+str(zero_num)+','+str(one_num)+']'+'\\n'+'class:'+str(the_class)\n",
    "    mytree = {content:{}}\n",
    "\n",
    "    # 如果到了叶子节点，则停止分裂\n",
    "    if bestFeature == -1:\n",
    "        return mytree\n",
    "    # 否则继续分裂\n",
    "    else:\n",
    "        depth = depth-1\n",
    "        left,right = splitByFeature(dataSet,bestFeature,split)\n",
    "        mytree[content]['T'] = creatTree_plot(left,features,depth,minSamples)\n",
    "        mytree[content]['F'] = creatTree_plot(right,features,depth,minSamples)\n",
    "        return mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c41f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据可视化\n",
    "def plot_model(tree, name):\n",
    "    g = Digraph(\"G\", filename=name, format='png', strict=False)\n",
    "    first_label = list(tree.keys())[0]\n",
    "    g.node(\"0\", first_label)\n",
    "    _sub_plot(g, tree, \"0\")\n",
    "    g.view()\n",
    "\n",
    "root = \"0\"\n",
    "def _sub_plot(g, tree, inc):\n",
    "    global root\n",
    "    first_label = list(tree.keys())[0]\n",
    "    ts = tree[first_label]\n",
    "    for i in ts.keys():\n",
    "        if isinstance(tree[first_label][i], dict):\n",
    "            root = str(int(root) + 1)\n",
    "            g.node(root, list(tree[first_label][i].keys())[0])\n",
    "            g.edge(inc, root, str(i))\n",
    "            _sub_plot(g, tree[first_label][i], root)\n",
    "        else:\n",
    "            root = str(int(root) + 1)\n",
    "            g.node(root, tree[first_label][i])\n",
    "            g.edge(inc, root, str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3c6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------- Predict----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda2c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatTree_predict(dataSet,features,depth,minSamples):\n",
    "\n",
    "    bestFeature, split, oldEntropy, sample_num, zero_num, one_num, node_class = findBestFeature(dataSet,features,depth,minSamples)\n",
    "    # 如果到了叶子节点\n",
    "    if bestFeature == -1:\n",
    "        content = node_class\n",
    "        mytree = {content:{}}\n",
    "        return mytree\n",
    "    \n",
    "    content = str(bestFeature)+'<='+str(split)\n",
    "    mytree = {content:{}}\n",
    "\n",
    "    # 如果到了叶子节点，则停止分裂\n",
    "    if bestFeature == -1:\n",
    "        return mytree\n",
    "    # 否则继续分裂\n",
    "    else:\n",
    "        depth = depth-1\n",
    "        left,right = splitByFeature(dataSet,bestFeature,split)\n",
    "        mytree[content]['T'] = creatTree_predict(left,features,depth,minSamples)\n",
    "        mytree[content]['F'] = creatTree_predict(right,features,depth,minSamples)\n",
    "        return mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeda2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theSplit(astring):\n",
    "    index=astring.find('=')\n",
    "    split=float(astring[index+1:])    \n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5bb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theFeature(astring):\n",
    "    index=astring.find('=')\n",
    "    feature=astring[:index-1]  \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b86d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClass(row,mytree):\n",
    "    theClass=0\n",
    "    for key in mytree:\n",
    "        if key==0 or key==1:\n",
    "            theClass=key\n",
    "        else:\n",
    "            feature=theFeature(key)\n",
    "            split=theSplit(key)\n",
    "            if row[feature]<=split:\n",
    "                theClass=findClass(row,mytree[key]['T'])\n",
    "            else:\n",
    "                theClass=findClass(row,mytree[key]['F'])\n",
    "    return theClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd40ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(testData,mytree):\n",
    "    realClass = np.array(testData['is_attributed'])\n",
    "    print('real',realClass)\n",
    "    predictClass = np.array([])\n",
    "    for i in range(0,len(testData)):\n",
    "        row=testData.loc[i].to_dict()\n",
    "        prediction = findClass(row,mytree)\n",
    "        predictClass = np.append(predictClass,prediction)\n",
    "    print(\"predict\")\n",
    "    print(predictClass)\n",
    "    temp = realClass-predictClass\n",
    "    num = sum(np.abs(temp))\n",
    "    \n",
    "    precision = 1-(num/len(testData))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b1ae073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------取一部分数据进行训练和预测-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f99563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet= pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac3ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ip', 'app', 'device', 'os', 'channel', 'click_hour', 'click_minute',\n",
       "       'click_second'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拆分click_time的时分秒(年月日都相同)\n",
    "dataSet['click_time']= pd.to_datetime(dataSet['click_time'])\n",
    "hour=[i.hour for i in dataSet['click_time']]\n",
    "minute=[i.minute for i in dataSet['click_time']]\n",
    "second=[i.second for i in dataSet['click_time']]\n",
    "\n",
    "# 处理数据集\n",
    "processed_data=dataSet.drop(columns=['click_time','attributed_time'])\n",
    "processed_data['click_hour']=hour\n",
    "processed_data['click_minute']=minute\n",
    "processed_data['click_second']=second\n",
    "\n",
    "# 取出features\n",
    "features=processed_data.columns\n",
    "features=np.delete(features,5)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5d49fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别为1的有 13275 条,类别为0的有 6986725 条\n",
    "len_1 = len(dataSet[dataSet[\"is_attributed\"]==1])\n",
    "len_0 = len(dataSet[dataSet[\"is_attributed\"]==0])\n",
    "index_1 = np.array(dataSet[dataSet[\"is_attributed\"]==1].index)\n",
    "index_0 = dataSet[dataSet[\"is_attributed\"]==0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "186014d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_method1_随机\n",
    "# depth = 2\n",
    "# num = 50000\n",
    "# index_tr = processed_data.index.tolist()\n",
    "# select_tr = np.array(random.sample(index_tr,num))\n",
    "# train_data = processed_data.loc[select_tr,:].reset_index()\n",
    "\n",
    "# select_method2_混合\n",
    "# 原数据集：类别为1的有 13275 条,类别为0的有 6986725 条\n",
    "# 由于数据偏斜性较大，训练数据的选择：选择所有类别为1的数据，类别为0的数据为随机选择，3倍,一共53100条训练数据\n",
    "depth = 2\n",
    "select = np.append(np.array(random.sample(index_0 ,len_1*3)),index_1)\n",
    "train_data = processed_data.loc[select,:]\n",
    "\n",
    "mytree_plot = creatTree_plot(train_data,features,depth,-1)\n",
    "plot_model(mytree_plot,\"output_31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aaac95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------预测-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9792a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理预测数据\n",
    "testData = pd.read_csv('test.csv')\n",
    "\n",
    "testData['click_time']= pd.to_datetime(testData['click_time'])\n",
    "hour=[i.hour for i in testData['click_time']]\n",
    "minute=[i.minute for i in testData['click_time']]\n",
    "second=[i.second for i in testData['click_time']]\n",
    "\n",
    "# 处理数据集\n",
    "processed_test_data=testData.drop(columns=['click_time','attributed_time'])\n",
    "processed_test_data['click_hour']=hour\n",
    "processed_test_data['click_minute']=minute\n",
    "processed_test_data['click_second']=second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "536284ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择test数据集（随机）\n",
    "num = 50000\n",
    "index = processed_test_data.index.tolist()\n",
    "select_t = np.array(random.sample(index ,num))\n",
    "test_data = processed_test_data.loc[select_t,:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4b74bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app<=18.5': {'T': {'app<=0.5': {'T': {1: {}}, 'F': {0: {}}}}, 'F': {'app<=19.5': {'T': {0: {}}, 'F': {0: {}}}}}}\n",
      "real [0 0 0 ... 0 0 0]\n",
      "predict\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "99.81%\n"
     ]
    }
   ],
   "source": [
    "# 进行预测，构造预测树的结构，train_data，features，depth同plot树的参数，只是树的组成结构不同\n",
    "mytree_predict = creatTree_predict(train_data,features,depth,-1)\n",
    "print(mytree_predict)\n",
    "precision = predict(test_data,mytree_predict)\n",
    "print(str(np.round(precision*100,2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9387415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
